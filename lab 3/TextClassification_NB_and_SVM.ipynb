{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 Newsgrous data set - a collection of 12k documents grouped among 20 different categories.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Find information about data set\n",
    "dir(twenty_train)\n",
    "print(twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'at', 'document', 'fmi', 'information', 'is', 'new', 'presented', 'retrieval', 'teaching', 'this']\n",
      "[[0 0 1 0 0 1 1 0 0 0 1]\n",
      " [1 1 1 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 1 1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Let's use the sklearn's feature CountVectorizer (Example)\n",
    "#Check the full documentation here: \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is a new document', 'another document presented at FMI', 'FMI is teaching information retrieval']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names()) # ALl vocabulary words found in corpus. \n",
    "#CHeck doc, you can set your own requested words\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "# Let's go back to extracting features from the real text\n",
    "# This will construct the vocabulary and document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "\n",
    "feature_names = count_vect.get_feature_names() # maps from feature index to string (word)\n",
    "#print(feature_names)\n",
    "\n",
    "inv_feature_names = { value:i for i,value in enumerate(feature_names) } \n",
    "#print(feature_names)\n",
    "#print(inv_feature_names)\n",
    "\n",
    "print(X_train_counts.shape) # Matrix of numDocuments X numFeatures (words)\n",
    "#print(X_train_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix X_train contains the raw term frequencies. We can manually compute the normalized tf by: raw_score(word) / total length of document\n",
    "# We can manually find TF-IDF by totalling columns where a word is != 0\n",
    "# But the code is already written for us !\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape\n",
    "#print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 4 ... 3 1 8]\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Run Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train first\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target) # Each row is a training entry, the corresponding index of twenty_train is category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7532, 130107)\n",
      "Predicted accuracy: 77.07116303770579%\n"
     ]
    }
   ],
   "source": [
    "# Let's check the performance\n",
    "import numpy as np\n",
    "\n",
    "# Load test data\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Convert the test data through the same pipeline as above\n",
    "count_vect = CountVectorizer(vocabulary = inv_feature_names) # IMportant thing - we have to use the same features (vocabulary words as before)\n",
    "X_test_counts = count_vect.fit_transform(twenty_test.data)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "\n",
    "print(X_test_tfidf.shape)\n",
    "\n",
    "# Predict data\n",
    "predicted = clf.predict(X_test_tfidf) # Will get an array of classifications\n",
    "res = np.mean(predicted == twenty_test.target) # Check how many succeded in average\n",
    "print(\"Predicted accuracy: {0}%\".format(res*100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cpaduraru\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accuracy: 82.38183749336166%\n"
     ]
    }
   ],
   "source": [
    "# More about it here - https://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn.pipeline import Pipeline  # Independent of SVM could be used for aboves too\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Define a pipeline to write less code\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), \n",
    "                    ('svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "\n",
    "svm_clf.fit(twenty_train.data, twenty_train.target)\n",
    "svm_predicted = svm_clf.predict(twenty_test.data)\n",
    "res = np.mean(svm_predicted == twenty_test.target)\n",
    "print(\"Predicted accuracy: {0}%\".format(res*100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Optimizing with Grid search \n",
    "### Searches for parameter inside models (you could implement something similar for homework 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-65-d6316f08c20c>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-65-d6316f08c20c>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    gridSearch_svm.best_params_\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range' : [(1,1), (1,2)], # Uni or bi gram ?\n",
    "                'tfidf__use_idf' : (True, False), # Either to use idf or not\n",
    "                'svm__alpha':(1e-2, 1e-3)}\n",
    "\n",
    "gridSearch_svm = GridSearchCV(svm_clf, parameters, n_jobs=-1)\n",
    "gridSearch_svm = gridSearch_svm.fit(twenty_train.data, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8979140887396146\n",
      "Best params: {'svm__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: {0}\".format( gridSearch_svm.best_score_))\n",
    "print(\"Best params: {0}\".format(gridSearch_svm.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 : Implement Naive bayes classifier yourself, similar to the one in the book. Use the same data set and make it close in terms of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here if you do it in python\n",
    "import numpy as np\n",
    "\n",
    "def trainMultinomialNB(trainMatrix, classOfDocument):\n",
    "    documentsOfClass = []\n",
    "    prior = []\n",
    "    T = []\n",
    "    nrOfClasses = 0\n",
    "    N, nrOfWords = trainMatrix.shape\n",
    "    # aflam numarul de clase\n",
    "    for val in classOfDocument:\n",
    "        if val > nrOfClasses:\n",
    "            nrOfClasses = val\n",
    "    nrOfClasses += 1\n",
    "    for _ in range(nrOfClasses):\n",
    "        documentsOfClass.append([])\n",
    "    T = np.zeros((nrOfClasses, nrOfWords))\n",
    "    condprob = np.zeros((nrOfClasses, nrOfWords))\n",
    "    # aflam documentele care apartin unei clase\n",
    "    for idx, val in enumerate(classOfDocument):\n",
    "        documentsOfClass[val].append(idx)\n",
    "    for c in range(nrOfClasses):\n",
    "        Nc = len(documentsOfClass[c])\n",
    "        prior.append(Nc/N)\n",
    "        # calculam Tct\n",
    "        for doc in documentsOfClass[c]:\n",
    "            T[c] += trainMatrix[doc]\n",
    "        sumOfClass = sum(T[c]) + nrOfWords\n",
    "        # calculam condprob\n",
    "        for word in range(nrOfWords):\n",
    "            condprob[c,word] = np.log10((T[c,word] + 1)/sumOfClass)\n",
    "    return prior, condprob\n",
    "\n",
    "prior, condprob = trainMultinomialNB(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMultinomialNB(prior, condprob, doc):\n",
    "    nrOfClasses, nrOfWords = condprob.shape\n",
    "    N, _ = doc.shape\n",
    "    arrayOfScores = []\n",
    "    for idxOfDoc in range(N):\n",
    "        score = []\n",
    "        for c in range(nrOfClasses):\n",
    "            score.append(np.log10(prior[c]))\n",
    "            score[c] += condprob[c] * np.transpose(doc[idxOfDoc])\n",
    "            #for word in range(nrOfWords):\n",
    "            #    score[c] += condprob[c,word] * doc[0, word]\n",
    "        maxim = score[0]\n",
    "        idxOfMax = 0\n",
    "        for c in range(nrOfClasses):\n",
    "            if score[c] > maxim:\n",
    "                maxim = score[c]\n",
    "                idxOfMax = c\n",
    "        arrayOfScores.append(idxOfMax)\n",
    "    return arrayOfScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accuracy: 77.07116303770579%\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "# Convert the test data through the same pipeline as above\n",
    "count_vect = CountVectorizer(vocabulary = inv_feature_names) # IMportant thing - we have to use the same features (vocabulary words as before)\n",
    "X_test_counts = count_vect.fit_transform(twenty_test.data)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "\n",
    "\n",
    "arrayOfScores = applyMultinomialNB(prior, condprob, X_test_tfidf)\n",
    "\n",
    "#print(arrayOfScores)\n",
    "#print(twenty_test.target)\n",
    "res2 = np.mean(arrayOfScores == twenty_test.target) # Check how many succeded in average\n",
    "print(\"Predicted accuracy: {0}%\".format(res2*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer and stop words tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cpaduraru\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted accuracy: 81.94370685077004%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer): # derivation from base class, need to rewrite a few functions\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "    \n",
    "# Define a pipeline to write less code\n",
    "svm_clf = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
    "                    ('svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "\n",
    "svm_clf.fit(twenty_train.data, twenty_train.target)\n",
    "svm_predicted = svm_clf.predict(twenty_test.data)\n",
    "res = np.mean(svm_predicted == twenty_test.target)\n",
    "print(\"Predicted accuracy: {0}%\".format(res*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
